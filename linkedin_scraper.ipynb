{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages for the project \n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import csv\n",
    "from random import randint\n",
    "import random as rand\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing a driver\n"
     ]
    }
   ],
   "source": [
    "# Login to Linkedin\n",
    "# Open Chrome and Access Linkedin login site\n",
    "driver = webdriver.Chrome()\n",
    "sleep(rand.uniform(1, 3))\n",
    "url = 'https://www.linkedin.com/login'\n",
    "driver.get(url)\n",
    "print('initializing a driver')\n",
    "sleep(rand.uniform(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing the login credentials\n"
     ]
    }
   ],
   "source": [
    "# Import username and password\n",
    "credential = open('credentials.txt')\n",
    "line = credential.readlines()\n",
    "username = line[0]\n",
    "password = line[1]\n",
    "print('importing the login credentials')\n",
    "sleep(rand.uniform(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Key in login credentials\n",
    "email_field = driver.find_element_by_id('username')\n",
    "email_field.send_keys(username)\n",
    "print('keying in email')\n",
    "sleep(rand.uniform(1, 3))\n",
    "\n",
    "password_field = driver.find_element_by_name('session_password')\n",
    "password_field.send_keys(password)\n",
    "print('keying in password')\n",
    "sleep(rand.uniform(1, 2))\n",
    "\n",
    "# Click the Login button\n",
    "signin_field = driver.find_element_by_xpath('//*[@id=\"organic-div\"]/form/div[3]/button')\n",
    "signin_field.click()\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in new table\n",
    "import csv\n",
    "file = open('linkpart9.csv')\n",
    "\n",
    "csvreader = csv.DictReader(file)\n",
    "URLs_all_page = [row['URL'] for row in csvreader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_URL='https://www.linkedin.com/in/jazm%C3%ADn-del-salto-content-media-specialist/'\n",
    "driver.get(linkedin_URL)\n",
    "\n",
    "page_source = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "info_div = page_source.find('div',{'class':'display-flex ph5 pv3'})\n",
    "info_div.find_all('span')[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the data of 1 Linkedin profile, and write the data to a .CSV file\n",
    "\n",
    "with open('output.csv', 'w',  newline = '') as file_output:\n",
    "    headers = ['URL', 'About']\n",
    "    writer = csv.DictWriter(file_output, delimiter=',', lineterminator='\\n',fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for linkedin_URL in URLs_all_page:\n",
    "        driver.get(linkedin_URL)\n",
    "        sleep(rand.uniform(1, 3))\n",
    "        print('scraping profile: ', linkedin_URL)\n",
    "        page_source = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        info_div = page_source.find('div',{'class':'display-flex ph5 pv3'})\n",
    "        about = ''\n",
    "        sleep(rand.uniform(1, 2))\n",
    "        if info_div:\n",
    "            print('found info')\n",
    "            info_about = info_div.find_all('span')\n",
    "            about = info_about[0].get_text().strip() #Remove unnecessary characters \n",
    "        print('--- Profile about is: ', about)\n",
    "        writer.writerow({headers[0]:linkedin_URL, headers[1]:about})\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging two tables on thw linkedin adress key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "about9= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/outputpart9.csv')  \n",
    "scores9 = pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/linkpart9.csv', index_col = False)\n",
    "ams9 = scores9.join(about9.set_index('URL'),how='inner', on='URL')\n",
    "ams9.to_csv(r'/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams9.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "ams1= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams1.csv')  \n",
    "ams2= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams2.csv')  \n",
    "ams3= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams3.csv')  \n",
    "ams4= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams4.csv')  \n",
    "ams5= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams5.csv')  \n",
    "ams6= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams6.csv')  \n",
    "ams7= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams7.csv')  \n",
    "ams8= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams8.csv')  \n",
    "ams9= pd.read_csv('/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/ams9.csv')  \n",
    "\n",
    "new = pd.concat([ams1,ams2,ams3,ams4,ams5,ams6,ams7,ams8,ams9], ignore_index=True)\n",
    "new.to_csv(r'/Users/mplome/dev/Gyfted/algorithms/knowledge_graph/indeed/table4model.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5ffb64ea58d46e3ac9f40c0486ca17668ebb7b8e6985b3435ef99c92c026b26"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('knowledge_graph-FjLIWlg-': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
