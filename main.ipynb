{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartynaPlomecka/DLM_DigitalFootprint/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c896a74-c3a2-4f35-b584-e5a3ad4c6834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr 21 16:02:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    27W /  70W |  13966MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "V1G82GuO-tez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41e5335-a8bf-4355-aaab-66abbfb79741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qXwgOm4wgULj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964b5122-8216-43ce-ff44-c440d6b95eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "## for data\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics, manifold\n",
        "## for processing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "## for bert\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "import transformers\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import sys\n",
        "import gzip\n",
        "import datasets\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd_ONue3gtFy"
      },
      "outputs": [],
      "source": [
        "# mount to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# dataset paths\n",
        "dataset_path = '/content/drive/My Drive/embeddings/table4model_clean.csv'\n",
        "not_labelled_indeed_path = '/content/drive/My Drive/embeddings/dev.txt'\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dtf = pd.read_csv(not_labelled_indeed_path, header = 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtf"
      ],
      "metadata": {
        "id": "zqDhpPLuDyhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtf_l = pd.read_csv(dataset_path, header = 0)\n",
        "labeled = dtf_l[['About', 'grit']]\n",
        "labeled"
      ],
      "metadata": {
        "id": "YoHhzZUpDUto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = labeled[0:400]\n",
        "val = labeled[400:550]\n",
        "test= labeled[550:]"
      ],
      "metadata": {
        "id": "0us0rG2v1ju6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_json = labeled.to_json()\n",
        "labeled.to_json('data.json', lines=True, orient='records')\n",
        "\n",
        "train_json = train.to_json()\n",
        "train.to_json('data_train.json', lines=True, orient='records')\n",
        "\n",
        "val_json = val.to_json()\n",
        "val.to_json('data_val.json', lines=True, orient='records')\n",
        "\n",
        "test_json = test.to_json()\n",
        "test.to_json('data_test.json', lines=True, orient='records')"
      ],
      "metadata": {
        "id": "MJ24w0DKWxJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_json('data.json')\n",
        "train_dataset = Dataset.from_json('data_train.json')\n",
        "val_dataset = Dataset.from_json('data_val.json')\n",
        "test_dataset = Dataset.from_json('data_test.json')"
      ],
      "metadata": {
        "id": "D3C7QVcKXXgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2nMGzuuD0GA"
      },
      "source": [
        "# masked language modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Masked Language Model (MLM) is the process how BERT was pre-trained. MLM is a powerful pre-training strategy for learning sentence embeddings. This is especially the case when we work on a specialized domain.\n",
        "\n"
      ],
      "metadata": {
        "id": "2B20fgveKwBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elKR3dZkEj9I"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import sys\n",
        "import gzip\n",
        "from datetime import datetime\n",
        "no_deprecation_warning=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWQ69aIXEpZz"
      },
      "outputs": [],
      "source": [
        "model_name = sys.argv[1]\n",
        "per_device_train_batch_size = 64\n",
        "\n",
        "save_steps = 1000               #Save model every 1k steps\n",
        "num_train_epochs = 3            #Number of epochs\n",
        "use_fp16 = False                #Set to True, if your GPU supports FP16 operations\n",
        "max_length = 100                #Max length for a text input\n",
        "do_whole_word_mask = True       #If set to true, whole words are masked\n",
        "mlm_prob = 0.15                 #Probability that a word is replaced by a [MASK] token\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM"
      ],
      "metadata": {
        "id": "KJ3mjLBKAsXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXgFZjVVE-ol"
      },
      "outputs": [],
      "source": [
        "\n",
        "#output_dir = \"output/{}-{}\".format(model_name.replace(\"/\", \"_\"),  datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "output_dir = \"output/model\"\n",
        "\n",
        "print(\"Save checkpoints to:\", output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaIBQmDcD0o-"
      },
      "outputs": [],
      "source": [
        "##### Load train dataset\n",
        "\n",
        "train_sentences = []\n",
        "train_path = '/content/drive/My Drive/embeddings/dev.txt'\n",
        "with gzip.open(train_path, 'rt', encoding='utf8') if train_path.endswith('.gz') else  open(train_path, 'r', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        line = line.strip()\n",
        "        if len(line) >= 10:\n",
        "            train_sentences.append(line)\n",
        "\n",
        "print(\"Train sentences:\", len(train_sentences))\n",
        "\n",
        "dev_sentences = []\n",
        "if len(sys.argv) >= 4:\n",
        "    dev_path = sys.argv[3]\n",
        "    with gzip.open(dev_path, 'rt', encoding='utf8') if dev_path.endswith('.gz') else open(dev_path, 'r', encoding='utf8') as fIn:\n",
        "        for line in fIn:\n",
        "            line = line.strip()\n",
        "            if len(line) >= 10:\n",
        "                dev_sentences.append(line)\n",
        "\n",
        "print(\"Dev sentences:\", len(dev_sentences))\n",
        "\n",
        "#A dataset wrapper, that tokenizes our data on-the-fly\n",
        "class TokenizedSentencesDataset:\n",
        "    def __init__(self, sentences, tokenizer, max_length, cache_tokenization=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sentences = sentences\n",
        "        self.max_length = max_length\n",
        "        self.cache_tokenization = cache_tokenization\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if not self.cache_tokenization:\n",
        "            return self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
        "\n",
        "        if isinstance(self.sentences[item], str):\n",
        "            self.sentences[item] = self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
        "        return self.sentences[item]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "train_dataset = TokenizedSentencesDataset(train_sentences, tokenizer, max_length)\n",
        "dev_dataset = TokenizedSentencesDataset(dev_sentences, tokenizer, max_length, cache_tokenization=True) if len(dev_sentences) > 0 else None\n",
        "\n",
        "##### Training arguments\n",
        "\n",
        "if do_whole_word_mask:\n",
        "    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
        "else:\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    evaluation_strategy=\"steps\" if dev_dataset is not None else \"no\",\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    eval_steps=save_steps,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=save_steps,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=use_fp16\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset\n",
        ")\n",
        "\n",
        "print(\"Save tokenizer to:\", output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Save model to:\", output_dir)\n",
        "model.save_pretrained(output_dir)\n",
        "\n",
        "print(\"Training done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/My Drive/embeddings/dev.txt'"
      ],
      "metadata": {
        "id": "08_C_EA4Qm5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model:"
      ],
      "metadata": {
        "id": "1a146bKJVcVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "state_dict = torch.load(\"output/model/pytorch_model.bin\") \n",
        "model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "SlTmLvlJb4wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, id2label=id2label, label2id=label2id)\n"
      ],
      "metadata": {
        "id": "2_O7IGnadYRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Datasets\n",
        "1.tokenising the dataset by calling tokenizer. \n",
        "2.associating the label attribute to each dataset item.\n"
      ],
      "metadata": {
        "id": "uKa4NGekdlLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "OxLfKAnohAtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is already labeled\n",
        "\n",
        "ds = {\"train\": train_dataset, \"validation\": val_dataset, \"test\": test_dataset}\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    label = examples[\"grit\"] \n",
        "    examples = tokenizer(examples[\"About\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "    examples[\"label\"] = float(label)\n",
        "    return examples\n",
        "\n",
        "for split in ds:\n",
        "    ds[split] = ds[split].filter(lambda e: e['grit'].isnumeric())\n",
        "    ds[split] = ds[split].map(preprocess_function, remove_columns=[\"About\", \"grit\"])"
      ],
      "metadata": {
        "id": "K4RkT4RzdtGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics\n",
        "We can compute metrics to track the model’s improvement during training. Here we retrieve the class with the highest logit (corresponding to the highest probability) for each prediction and compare it with the actual label to calculate the global accuracy score."
      ],
      "metadata": {
        "id": "JtvGjzp1eA_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "ehCyP_RaeRHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We put the output directory for the trained model and the learning parameters into TrainingArguments. With load_best_model_at_end and metric_for_best_model, we will keep several best models (i.e. those with the highest accuracy on the validation set) during training and load the best model at the end."
      ],
      "metadata": {
        "id": "sKVWl3_ReWJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"../models/camembert-fine-tuned-regression\",\n",
        "    learning_rate=1e-3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=3,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    load_best_model_at_end=True,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "metadata": {
        "id": "k-JLJGV_eaJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def compute_metrics_for_regression(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.reshape(-1, 1)\n",
        "    \n",
        "    mse = mean_squared_error(labels, logits)\n",
        "    mae = mean_absolute_error(labels, logits)\n",
        "    r2 = r2_score(labels, logits)\n",
        "    single_squared_errors = ((logits - labels).flatten()**2).tolist()\n",
        "    \n",
        "    # Compute accuracy \n",
        "    # Based on the fact that the rounded score = true score only if |single_squared_errors| < 0.5\n",
        "    accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n",
        "    \n",
        "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2, \"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "lyzI8qU6l0mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "class RegressionTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        #print(outputs[0][:, 0].size())\n",
        "        #a = torch.nn.Linear(30522, 1).cuda()\n",
        "        #logits = a(outputs[0][:, 0].cuda())\n",
        "        logits = outputs[0][0, 0][0]\n",
        "        loss = torch.nn.functional.mse_loss(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "TNhs8DkikadR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "trainer = RegressionTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"validation\"],\n",
        "    compute_metrics=compute_metrics_for_regression,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "WWGvihzheeXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "Note that we rely on the validation set’s accuracy to retrieve the best model. Calling Trainer.evaluate(), we can retrieve the best accuracy attained during training, which is 0.683 (at epoch 16).\n"
      ],
      "metadata": {
        "id": "HTZRx89mekpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "NKNU48Zeeq6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "main",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}